{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder with single hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X,size):\n",
    "    return X[np.random.choice(len(X), size, replace = False)]\n",
    "\n",
    "class Autoencoder:\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size = 10, epoch=250, learning_rate=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        x = tf.placeholder(tf.float32,[None, input_dim])    # N_samples x N_features\n",
    "        \n",
    "        with tf.name_scope('encoder'):\n",
    "            weights = tf.Variable(tf.random_normal([input_dim, hidden_dim]), dtype = tf.float32, name='weights') # N_features x N_hidden\n",
    "            bias = tf.Variable(tf.zeros([1,hidden_dim]), dtype = tf.float32, name = 'bias') # 1 x N_hidden\n",
    "            encoded = tf.nn.tanh(x @ weights + bias) # N_samples x N_hidden\n",
    "        with tf.name_scope('decoder'):\n",
    "            weights = tf.Variable(tf.random_normal([ hidden_dim, input_dim]), dtype = tf.float32, name='weights') # N_hiddenx N_features\n",
    "            bias = tf.Variable(tf.zeros([1,input_dim]), dtype = tf.float32, name = 'bias')  # 1 x N_features\n",
    "            decoded = (encoded @ weights + bias) # N_samples x N_features\n",
    "        \n",
    "        self.x = x\n",
    "        self.encoded = encoded\n",
    "        self.decoded = decoded\n",
    "            \n",
    "        self.cost = tf.sqrt(tf.reduce_mean(tf.square(self.x-self.decoded)))\n",
    "        self.train_op = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.cost)\n",
    "        # Gradient descent is slowest: Here are other choices -\n",
    "        # tf.train.AdagradOptimizer, tf.train.AdagradDAOptimizer,\n",
    "        # tf.train.MomentumOptimizer, tf.train.AdamOptimizer, tf.train.FtrlOptimizer\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def train(self, data):\n",
    "        num_samples = len(data)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(self.epoch):\n",
    "                for j in range(500):\n",
    "                    batch_data = get_batch(data, self.batch_size)\n",
    "                    loss, _ = sess.run([self.cost, self.train_op], feed_dict={self.x : batch_data})\n",
    "                if i % 10 ==0: \n",
    "                    print('epoch {} : loss = {}' .format(i,loss))\n",
    "                    self.saver.save(sess,'./model.ckpt')\n",
    "            self.saver.save(sess,'./model.ckpt')\n",
    "        \n",
    "    def test(self, data):\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess,'./model.ckpt')\n",
    "            hidden, reconstructed = sess.run([self.encoded, self.decoded], feed_dict = {self.x : data})\n",
    "            print('Input : ', data)\n",
    "            print('Compressed : ', hidden)\n",
    "            print('Reconstructed : ', reconstructed)\n",
    "        return reconstructed\n",
    "                    \n",
    "                    \n",
    "                               \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 2\n",
    "data = datasets.load_iris().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From G:\\Anaconda\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "epoch 0 : loss = 3.434065103530884\n",
      "epoch 10 : loss = 0.3701309263706207\n",
      "epoch 20 : loss = 0.23624461889266968\n",
      "epoch 30 : loss = 0.23182491958141327\n",
      "epoch 40 : loss = 0.14751219749450684\n",
      "epoch 50 : loss = 0.16832223534584045\n",
      "epoch 60 : loss = 0.1880117505788803\n",
      "epoch 70 : loss = 0.17619356513023376\n",
      "epoch 80 : loss = 0.15801282227039337\n",
      "epoch 90 : loss = 0.17461101710796356\n",
      "epoch 100 : loss = 0.18727660179138184\n",
      "epoch 110 : loss = 0.11468915641307831\n",
      "epoch 120 : loss = 0.163792222738266\n",
      "epoch 130 : loss = 0.18811912834644318\n",
      "epoch 140 : loss = 0.16835914552211761\n",
      "epoch 150 : loss = 0.2223578244447708\n",
      "epoch 160 : loss = 0.18948659300804138\n",
      "epoch 170 : loss = 0.18121179938316345\n",
      "epoch 180 : loss = 0.12824703752994537\n",
      "epoch 190 : loss = 0.1723240166902542\n",
      "epoch 200 : loss = 0.15339426696300507\n",
      "epoch 210 : loss = 0.16876456141471863\n",
      "epoch 220 : loss = 0.19350369274616241\n",
      "epoch 230 : loss = 0.14975221455097198\n",
      "epoch 240 : loss = 0.22511959075927734\n",
      "WARNING:tensorflow:From G:\\Anaconda\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "Input :  [[8, 4, 6, 2]]\n",
      "Compressed :  [[ 0.3006713  -0.61862475]]\n",
      "Reconstructed :  [[7.7336464 3.8457012 5.8278704 2.0732484]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.7336464, 3.8457012, 5.8278704, 2.0732484]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(data[0])\n",
    "ae = Autoencoder(input_dim, hidden_dim)\n",
    "ae.train(data)\n",
    "ae.test([[8, 4, 6, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "names = unpickle('./cifar-10-batches-py/batches.meta')['label_names']\n",
    "data, labels = [], []\n",
    "for i in range(1, 6):\n",
    "    filename = './cifar-10-batches-py/data_batch_' + str(i)\n",
    "    batch_data = unpickle(filename)\n",
    "    if len(data) > 0:\n",
    "        data = np.vstack((data, batch_data['data']))\n",
    "        labels = np.hstack((labels, batch_data['labels']))\n",
    "    else:\n",
    "        data = batch_data['data']\n",
    "        labels = batch_data['labels']\n",
    "        \n",
    "def grayscale(a):\n",
    "    return a.reshape(a.shape[0], 3, 32, 32).mean(1).reshape(a.shape[0], -1)\n",
    "\n",
    "data = grayscale(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1024)\n",
      "epoch 0 : loss = 94.04011535644531\n",
      "epoch 10 : loss = 52.74992752075195\n",
      "epoch 20 : loss = 54.91603469848633\n",
      "epoch 30 : loss = 55.78679656982422\n",
      "epoch 40 : loss = 49.97771072387695\n",
      "epoch 50 : loss = 57.22572326660156\n",
      "epoch 60 : loss = 46.68358612060547\n",
      "epoch 70 : loss = 47.60668182373047\n",
      "epoch 80 : loss = 54.73023986816406\n",
      "epoch 90 : loss = 43.55459213256836\n",
      "epoch 100 : loss = 49.520469665527344\n",
      "epoch 110 : loss = 48.13953399658203\n",
      "epoch 120 : loss = 47.695072174072266\n",
      "epoch 130 : loss = 45.13964080810547\n",
      "epoch 140 : loss = 48.41259002685547\n",
      "epoch 150 : loss = 46.515403747558594\n",
      "epoch 160 : loss = 48.10332107543945\n",
      "epoch 170 : loss = 47.239131927490234\n",
      "epoch 180 : loss = 47.88938522338867\n",
      "epoch 190 : loss = 53.45363235473633\n"
     ]
    }
   ],
   "source": [
    "x = np.matrix(data)\n",
    "y = np.array(labels)\n",
    "horse_indices = np.where(y == 7)[0]\n",
    "horse_x = x[horse_indices]\n",
    "print(np.shape(horse_x)) # (5000, 3072)\n",
    "input_dim = np.shape(horse_x)[1]\n",
    "hidden_dim = 100\n",
    "ae = Autoencoder(input_dim, hidden_dim,epoch=200)\n",
    "ae.train(horse_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165.33333333, 154.33333333, 163.33333333, ...,  83.33333333,\n",
       "         81.66666667,  79.33333333]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[np.newaxis,512,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "Input :  [[165.33333333 154.33333333 163.33333333 ...  83.33333333  81.66666667\n",
      "   79.33333333]]\n",
      "Compressed :  [[ 1.  1. -1. -1.  1. -1.  1. -1.  1. -1.  1.  1.  1. -1. -1.  1. -1. -1.\n",
      "  -1.  1.  1. -1. -1. -1.  1.  1. -1. -1. -1.  1. -1.  1.  1.  1.  1. -1.\n",
      "  -1. -1. -1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1. -1.  1.\n",
      "  -1. -1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.\n",
      "  -1. -1.  1. -1.  1. -1.  1.  1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1.\n",
      "   1. -1. -1. -1.  1. -1. -1. -1. -1.  1.]]\n",
      "Reconstructed :  [[150.61594  150.97699  152.04547  ...  70.15095   72.889755  72.77891 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[150.61594 , 150.97699 , 152.04547 , ...,  70.15095 ,  72.889755,\n",
       "         72.77891 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.test(data[np.newaxis,512,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
